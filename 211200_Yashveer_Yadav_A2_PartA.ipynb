{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StochasticMazeEnv(gym.Env):\n",
    "\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self,initial_state=0,no_states=12,no_actions=4):\n",
    "        \n",
    "        self.initial_state = initial_state\n",
    "        self.state = self.initial_state\n",
    "        self.nA = no_actions\n",
    "        self.nS = no_states\n",
    "        self.actions_dict = {\"L\":0, \"U\":1, \"R\":2, \"D\":3}\n",
    "        self.prob_dynamics = {\n",
    "            # Write what does this MDP dynamics show !!!!\n",
    "            0: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                2: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                1: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "                2: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                3: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(0.8, 1, -0.01, False), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "                2: [(0.8, 3, +1, True), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                3: [(0.8, 6, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                1: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                2: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                3: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "            },\n",
    "            4: {\n",
    "                0: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                2: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "            },\n",
    "            6: {\n",
    "                0: [(0.8, 6, -0.01, False), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "                2: [(0.8, 7, -1, True), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "            },\n",
    "            7: {\n",
    "                0: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                1: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                2: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                3: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "            },\n",
    "            8: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 4, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                2: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "            },\n",
    "            9: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                1: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                2: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                3: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            },\n",
    "            10: {\n",
    "                0: [(0.8, 9, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 6, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)]\n",
    "            },\n",
    "            11: {\n",
    "                0: [(0.8, 10, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                1: [(0.8, 7, -1, True), (0.1, 10, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                3: [(0.8, 11, -0.01, False), (0.1, 11, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            }\n",
    "        }\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.initial_state\n",
    "        return self.get_obs()\n",
    "\n",
    "    def get_obs(self):\n",
    "        return (self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(\"Current state: {}\".format(self.state))\n",
    "\n",
    "    def sample_action(self):\n",
    "        return random.randint(0, self.nA)\n",
    "    \n",
    "    def P(self):\n",
    "        \n",
    "        self.prob_dynamics = {\n",
    "            0: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                2: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                1: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "                2: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                3: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(0.8, 1, -0.01, False), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "                2: [(0.8, 3, +1, True), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                3: [(0.8, 6, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                1: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                2: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                3: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "            },\n",
    "            4: {\n",
    "                0: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                2: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "            },\n",
    "            6: {\n",
    "                0: [(0.8, 6, -0.01, False), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "                2: [(0.8, 7, -1, True), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "            },\n",
    "            7: {\n",
    "                0: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                1: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                2: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                3: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "            },\n",
    "            8: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 4, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                2: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "            },\n",
    "            9: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                1: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                2: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                3: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            },\n",
    "            10: {\n",
    "                0: [(0.8, 9, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 6, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)]\n",
    "            },\n",
    "            11: {\n",
    "                0: [(0.8, 10, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                1: [(0.8, 7, -1, True), (0.1, 10, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                3: [(0.8, 11, -0.01, False), (0.1, 11, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            }\n",
    "        }\n",
    "        return self.prob_dynamics\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        '''\n",
    "        Performs the given action\n",
    "        Args:\n",
    "            action : action from the action_space to be taking in the environment\n",
    "        Returns:\n",
    "            observation - returns current state\n",
    "            reward - reward obtained after taking the given action\n",
    "            done - True if the episode is complete else False\n",
    "        '''\n",
    "        if action >= self.nA:\n",
    "            action = self.nA-1\n",
    "\n",
    "        index = np.random.choice(3,1,p=[0.8,0.1,0.1])[0]\n",
    "\n",
    "        dynamics_tuple = self.prob_dynamics[self.state][action][index]\n",
    "        self.state = dynamics_tuple[1]\n",
    "        \n",
    "\n",
    "        return self.state, dynamics_tuple[2], dynamics_tuple[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My policy is this >>>>>\n",
      "[[ 0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [-1. -1. -1. -1.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.]]\n",
      "\n",
      "The best policy is :\n",
      "State: 0 Action: Right\n",
      "State: 1 Action: Right\n",
      "State: 2 Action: Right\n",
      "State: 4 => Episode Ends :)\n",
      "State: 4 Action: Up\n",
      "State: 5 => This is Wall\n",
      "State: 6 Action: Up\n",
      "State: 7 Action: Left\n",
      "State: 8 Action: Up\n",
      "State: 9 Action: Right\n",
      "State: 10 Action: Up\n",
      "State: 11 Action: Left\n"
     ]
    }
   ],
   "source": [
    "# Policy Iteration \n",
    "class PolicyIterationAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.nS = env.nS\n",
    "        self.nA = env.nA\n",
    "        # Creating a policy matrix with each row representing a state.\n",
    "        # And each column representing an action.\n",
    "        # Initially all actions for all state are considered to be of equal probability creating a uniform distribution.\n",
    "        self.policy = np.zeros((self.nS, self.nA))  # Initialize a policy in which we are taking no action Or in deterministic term taking any action at random\n",
    "\n",
    "    # Defining the policy evaluation function.\n",
    "    def policy_evaluation(self, policy, gamma=0.9, theta=0.001):\n",
    "        # Initialising the Value array having elements = number of states and having initially all zero elements.\n",
    "        V = np.zeros(self.nS)\n",
    "\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(self.nS):\n",
    "                v = V[s]\n",
    "                if s == 5:  \n",
    "                    # For state 5 (the wall), set the value to -10 because we will hit the wall and we know that it doesn't give any benefit\n",
    "                    # because intial state remain same as the final state.\n",
    "                    V[s] = 0\n",
    "                else:\n",
    "                    # This calculates the value of state ('V[s]') using Boolean equation.\n",
    "                    # For a in range(self.nA) this iterates over all actions a in the given state s.\n",
    "                    # Calculating V[s] using the bellman expectation equation.\n",
    "                    # Summing over all possible states and all possible actions.\n",
    "                    V[s] = sum(policy[s, a] * sum(p * (r + gamma * V[s_prime]) for p, s_prime, r, _ in self.env.prob_dynamics[s][a]) for a in range(self.nA))\n",
    "                # just to calculate the convergence.\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            \n",
    "            # If delta is less than theta then we have to break assuming that we converged to the optimal policy\n",
    "            if delta < theta:\n",
    "                break\n",
    "        # Returning the value array for the policy.\n",
    "        return V\n",
    "\n",
    "    # Doing the policy improvement.\n",
    "    def policy_improvement(self, V, gamma=0.9):\n",
    "        policy_stable = True\n",
    "        for s in range(self.nS):\n",
    "            old_action = np.argmax(self.policy[s])\n",
    "\n",
    "            if s == 5:  # For state 5 (the wall), we will never be at that state\n",
    "                self.policy[s] = np.full_like(self.nA,-1)\n",
    "                continue\n",
    "\n",
    "            action_values = np.zeros(self.nA)\n",
    "            # iterating over all possible action and taking the one with maximum value for state S.\n",
    "            for a in range(self.nA):\n",
    "                action_values[a] = sum(p * (r + gamma * V[s_prime]) for p, s_prime, r, _ in self.env.prob_dynamics[s][a])\n",
    "            # finding the best action for state s\n",
    "            best_action = np.argmax(action_values)\n",
    "            if(s!=5):\n",
    "                # Updatign the policy by making the probability of best action as one and rest as 0\n",
    "                self.policy[s] = np.eye(self.nA)[best_action]\n",
    "\n",
    "            # If old action is not same as the best action , this implies that we have not yet converged to the best policy.\n",
    "            if old_action != best_action:\n",
    "                policy_stable = False\n",
    "        \n",
    "        return policy_stable\n",
    "\n",
    "    def policy_iteration(self, gamma=0.9, max_iterations=1000):\n",
    "        for _ in range(max_iterations):\n",
    "            V = self.policy_evaluation(self.policy, gamma)\n",
    "            policy_stable = self.policy_improvement(V, gamma)\n",
    "            # If we have reached the optimal policy then break.\n",
    "            if policy_stable:\n",
    "                break\n",
    "\n",
    "        return self.policy\n",
    "\n",
    "# Usage\n",
    "env = StochasticMazeEnv()\n",
    "agent = PolicyIterationAgent(env)\n",
    "optimal_policy = agent.policy_iteration()\n",
    "my_dict= {-1 : \"Wall\" ,0 : \"Left\", 1: \"Up\" , 2: \"Right\" , 4: \"Down\"}\n",
    "print(\"My policy is this >>>>>\")\n",
    "print(optimal_policy)\n",
    "print()\n",
    "print(\"The best policy is :\")\n",
    "for s in range(0,12):\n",
    "    if(s==5):\n",
    "        print(\"State: 5 => This is Wall\")\n",
    "    elif(s==3):\n",
    "        print(\"State: 4 => Episode Ends :)\")\n",
    "    else:\n",
    "        print(\"State:\",s,\"Action:\",my_dict[np.argmax(optimal_policy[s])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The policy is [ 2.  2.  2. -1.  1. -1.  1.  0.  1.  2.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Value iteration\n",
    "class Valueiterationagent:\n",
    "    def __init__(self,env) :\n",
    "        self.env = env\n",
    "        self.nS = env.nS\n",
    "        self.nA = env.nA\n",
    "        self.policy=np.zeros(self.nS)\n",
    "    # Evaluating the value using V[s]= maxa(sum over s'(p(s')*(r+V[s'])))\n",
    "    # Continuing this evauluation till the V converges      \n",
    "    def value_evaluation(self,gamma=0.9,theta=0.001):\n",
    "        V=np.zeros(self.nS)\n",
    "        while True:\n",
    "            delta=0.0\n",
    "            for s in range(self.nS):\n",
    "                if(s==5):\n",
    "                    continue\n",
    "                v=V[s]\n",
    "                for a in range(self.nA):\n",
    "                    sum=0\n",
    "                    for p, s_prime, r, _ in self.env.prob_dynamics[s][a]:\n",
    "                        sum+=(p*(r+gamma*V[s_prime]))\n",
    "                    V[s]=max(V[s],sum)\n",
    "                \n",
    "                delta=max(delta,abs(v-V[s]))\n",
    "            \n",
    "            if(delta<theta):\n",
    "                break\n",
    "        return V\n",
    "\n",
    "    # Performing the Value Iteration.\n",
    "    def value_iteration(self,gamma=0.9):\n",
    "        # Calculating the value for each state using the value_evaluation function\n",
    "        V = self.value_evaluation(gamma)\n",
    "        self.policy[5]=-1\n",
    "        # Finding the Deterministic Policy by rule policy[s]= argmax a (sum over s'(p*(r+gamma*V[s'])))\n",
    "        for s in range(self.nS):\n",
    "            if(s==5):\n",
    "                continue\n",
    "            prev_sum=0\n",
    "            for a in range(self.nA):\n",
    "                sum=0\n",
    "                for p,s_prime,r,_ in self.env.prob_dynamics[s][a]:\n",
    "                    sum+=(p*(r+gamma*V[s_prime]))\n",
    "                if(sum>prev_sum):\n",
    "                    self.policy[s]=a\n",
    "                    prev_sum=sum\n",
    "        # because if initial state is state 3 then episode will end instantanously\n",
    "        self.policy[3]=-1        \n",
    "        print(\"The policy is\" ,self.policy)\n",
    "                \n",
    "\n",
    "env = StochasticMazeEnv()\n",
    "agent = Valueiterationagent(env)\n",
    "optimal_policy = agent.value_iteration()\n",
    "# In the Policy -1 is shown for the wall because this state is never possible\n",
    "# In the Policy -1 is shown for the Goal because , If our initial state is the Goal then the episode will end and we will not take any action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
